{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/kylemcdonald/Parametric-t-SNE/blob/master/Parametric%20t-SNE%20(Keras).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_data/env-py3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/local_data/env-py3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as k\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "k.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 230 ms, sys: 10.1 ms, total: 240 ms\n",
      "Wall time: 253 ms\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "\n",
    "%time (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hbeta(D, beta):\n",
    "    P = np.exp(-D * beta)\n",
    "    sumP = np.sum(P)\n",
    "    H = np.log(sumP) + beta * np.sum(np.multiply(D, P)) / sumP\n",
    "    P = P / sumP\n",
    "    return H, P\n",
    "\n",
    "def x2p(X, u=15, tol=1e-4, print_iter=500, max_tries=50, verbose=0):\n",
    "    # Initialize some variables\n",
    "    n = X.shape[0]                     # number of instances\n",
    "    P = np.zeros((n, n))               # empty probability matrix\n",
    "    beta = np.ones(n)                  # empty precision vector\n",
    "    logU = np.log(u)                   # log of perplexity (= entropy)\n",
    "    \n",
    "    # Compute pairwise distances\n",
    "    if verbose > 0: print('Computing pairwise distances...')\n",
    "    sum_X = np.sum(np.square(X), axis=1)\n",
    "    # note: translating sum_X' from matlab to numpy means using reshape to add a dimension\n",
    "    D = sum_X + sum_X[:,None] + -2 * X.dot(X.T)\n",
    "\n",
    "    # Run over all datapoints\n",
    "    if verbose > 0: print('Computing P-values...')\n",
    "    for i in range(n):\n",
    "        \n",
    "        if verbose > 1 and print_iter and i % print_iter == 0:\n",
    "            print('Computed P-values {} of {} datapoints...'.format(i, n))\n",
    "        \n",
    "        # Set minimum and maximum values for precision\n",
    "        betamin = float('-inf')\n",
    "        betamax = float('+inf')\n",
    "        \n",
    "        # Compute the Gaussian kernel and entropy for the current precision\n",
    "        indices = np.concatenate((np.arange(0, i), np.arange(i + 1, n)))\n",
    "        Di = D[i, indices]\n",
    "        H, thisP = Hbeta(Di, beta[i])\n",
    "        \n",
    "        # Evaluate whether the perplexity is within tolerance\n",
    "        Hdiff = H - logU\n",
    "        tries = 0\n",
    "        while abs(Hdiff) > tol and tries < max_tries:\n",
    "            \n",
    "            # If not, increase or decrease precision\n",
    "            if Hdiff > 0:\n",
    "                betamin = beta[i]\n",
    "                if np.isinf(betamax):\n",
    "                    beta[i] *= 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamax) / 2\n",
    "            else:\n",
    "                betamax = beta[i]\n",
    "                if np.isinf(betamin):\n",
    "                    beta[i] /= 2\n",
    "                else:\n",
    "                    beta[i] = (beta[i] + betamin) / 2\n",
    "            \n",
    "            # Recompute the values\n",
    "            H, thisP = Hbeta(Di, beta[i])\n",
    "            Hdiff = H - logU\n",
    "            tries += 1\n",
    "        \n",
    "        # Set the final row of P\n",
    "        P[i, indices] = thisP\n",
    "        \n",
    "    if verbose > 0: \n",
    "        print('Mean value of sigma: {}'.format(np.mean(np.sqrt(1 / beta))))\n",
    "        print('Minimum value of sigma: {}'.format(np.min(np.sqrt(1 / beta))))\n",
    "        print('Maximum value of sigma: {}'.format(np.max(np.sqrt(1 / beta))))\n",
    "    \n",
    "    return P, beta\n",
    "\n",
    "def compute_joint_probabilities(samples, batch_size=5000, d=2, perplexity=30, tol=1e-5, verbose=0):\n",
    "    v = d - 1\n",
    "    \n",
    "    # Initialize some variables\n",
    "    n = samples.shape[0]\n",
    "    batch_size = min(batch_size, n)\n",
    "    \n",
    "    # Precompute joint probabilities for all batches\n",
    "    if verbose > 0: print('Precomputing P-values...')\n",
    "    batch_count = int(n / batch_size)\n",
    "    P = np.zeros((batch_count, batch_size, batch_size))\n",
    "    for i, start in enumerate(range(0, n - batch_size + 1, batch_size)):   \n",
    "        curX = samples[start:start+batch_size]                   # select batch\n",
    "        P[i], beta = x2p(curX, perplexity, tol, verbose=verbose) # compute affinities using fixed perplexity\n",
    "        P[i][np.isnan(P[i])] = 0                                 # make sure we don't have NaN's\n",
    "        P[i] = (P[i] + P[i].T) # / 2                             # make symmetric\n",
    "        P[i] = P[i] / P[i].sum()                                 # obtain estimation of joint probabilities\n",
    "        P[i] = np.maximum(P[i], np.finfo(P[i].dtype).eps)\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing P-values...\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.2103958967227415\n",
      "Minimum value of sigma: 1.1465189822400297\n",
      "Maximum value of sigma: 3.423562241637291\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.2133319923532793\n",
      "Minimum value of sigma: 1.028104158803783\n",
      "Maximum value of sigma: 3.3874801273632515\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.2543639179071384\n",
      "Minimum value of sigma: 1.0993403688963808\n",
      "Maximum value of sigma: 3.6720262251663818\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.21620780986227\n",
      "Minimum value of sigma: 1.0813968241164726\n",
      "Maximum value of sigma: 3.330195765189976\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.247200217612878\n",
      "Minimum value of sigma: 1.061683289843211\n",
      "Maximum value of sigma: 3.476759114723516\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.181038022257536\n",
      "Minimum value of sigma: 0.9031738375276179\n",
      "Maximum value of sigma: 3.6655478368517804\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.1962136203081633\n",
      "Minimum value of sigma: 1.0022621389059558\n",
      "Maximum value of sigma: 3.4206718951588666\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.2167373801815025\n",
      "Minimum value of sigma: 1.0503807230236393\n",
      "Maximum value of sigma: 3.3508994412507405\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.200239975087905\n",
      "Minimum value of sigma: 1.1353135977722875\n",
      "Maximum value of sigma: 3.3820576890069067\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.213446139373871\n",
      "Minimum value of sigma: 1.0805969136944074\n",
      "Maximum value of sigma: 3.3211063109107593\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.1843809626340946\n",
      "Minimum value of sigma: 1.1253970429156568\n",
      "Maximum value of sigma: 3.3997426401552677\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.21174432211581\n",
      "Minimum value of sigma: 1.0710312729853213\n",
      "Maximum value of sigma: 3.586912803113833\n",
      "CPU times: user 1min 42s, sys: 22.7 s, total: 2min 4s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%time P = compute_joint_probabilities(X_train, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 5000, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for p in P:\n",
    "    tmp = np.allclose(p, p.T)\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### store / load distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1.01 s, total: 1.01 s\n",
      "Wall time: 989 ms\n"
     ]
    }
   ],
   "source": [
    "%time np.save('P.npy', P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 978 ms, total: 978 ms\n",
      "Wall time: 977 ms\n"
     ]
    }
   ],
   "source": [
    "%time P = np.load('P.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tsne function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_8:0' shape=(10, 10) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "x = K.variable(1 - np.eye(10))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<tf.Variable 'Variable_7:0' shape=(10, 10) dtype=float32_ref>,\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P is the joint probabilities for this batch (Keras loss functions call this y_true)\n",
    "# activations is the low-dimensional output (Keras loss functions call this y_pred)\n",
    "def tsne(P, activations):\n",
    "#     d = K.shape(activations)[1]\n",
    "    d = 2 # TODO: should set this automatically, but the above is very slow for some reason\n",
    "    n = batch_size # TODO: should set this automatically\n",
    "    v = d -  1.\n",
    "    eps = K.variable(10e-15) # needs to be at least 10e-8 to get anything after Q /= K.sum(Q)\n",
    "    sum_act = K.sum(K.square(activations), axis=1)\n",
    "    Q = K.reshape(sum_act, [-1, 1]) + -2 * K.dot(activations, K.transpose(activations))\n",
    "    Q = (sum_act + Q) / v\n",
    "    Q = K.pow(1 + Q, -(v + 1) / 2)\n",
    "    print(n)\n",
    "    Q *= K.variable(1 - np.eye(n))\n",
    "    Q /= K.sum(Q)\n",
    "    Q = K.maximum(Q, eps)\n",
    "    C = K.log((P + eps) / (Q + eps))\n",
    "    C = K.sum(P * C)\n",
    "    #return C\n",
    "    return K.constant(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_DIM = 2\n",
    "def KLdivergence(P, Y):\n",
    "    alpha = LOW_DIM - 1.\n",
    "    sum_Y = K.sum(K.square(Y), axis=1)\n",
    "    eps = K.variable(10e-15)\n",
    "    D = sum_Y + K.reshape(sum_Y, [-1, 1]) - 2 * K.dot(Y, K.transpose(Y))\n",
    "    Q = K.pow(1 + D / alpha, -(alpha + 1) / 2)\n",
    "    Q *= K.variable(1 - np.eye(batch_size))\n",
    "    Q /= K.sum(Q)\n",
    "    Q = K.maximum(Q, eps)\n",
    "    C = K.log((P + eps) / (Q + eps))\n",
    "    C = K.sum(P * C)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EPS = 1e-7\n",
    "def _get_normed_sym_tf(X_, batch_size):\n",
    "    \"\"\"\n",
    "    Compute the normalized and symmetrized probability matrix from\n",
    "    relative probabilities X_, where X_ is a Tensorflow Tensor\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_ : 2-d Tensor (N, N)\n",
    "        asymmetric probabilities. For instance, X_(i, j) = P(i|j)\n",
    "    Returns\n",
    "    -------\n",
    "    P : 2-d Tensor (N, N)\n",
    "        symmetric probabilities, making the assumption that P(i|j) = P(j|i)\n",
    "        Diagonals are all 0s.\"\"\"\n",
    "    toset = tf.constant(0, shape=[batch_size], dtype=X_.dtype)\n",
    "    X_ = tf.matrix_set_diag(X_, toset)\n",
    "    norm_facs = tf.reduce_sum(X_, axis=0, keep_dims=True)\n",
    "    X_ = X_ / norm_facs\n",
    "    X_ = 0.5*(X_ + tf.transpose(X_))\n",
    "    \n",
    "    return X_\n",
    "\n",
    "def _get_squared_cross_diff_tf(X_):\n",
    "    \"\"\"Compute squared differences of sample data vectors.\n",
    "    Implementation for Tensorflow Tensors\n",
    "    Z_ij = ||x_i - x_j||^2, where x_i = X_[i, :]\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_ : 2-d Tensor, (N, D)\n",
    "        Calculates outer vector product\n",
    "        This is the current batch of input data; `batch_size` x `dimension`\n",
    "    Returns\n",
    "    -------\n",
    "    Z_ij: 2-d Tensor, (N, N)\n",
    "        `batch_size` x `batch_size`\n",
    "        Tensor of squared differences between x_i and x_j\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(X_)[0]\n",
    "    \n",
    "    expanded = tf.expand_dims(X_, 1)\n",
    "    # \"tiled\" is now stacked up all the samples along dimension 1\n",
    "    tiled = tf.tile(expanded, tf.stack([1, batch_size, 1]))\n",
    "    \n",
    "    tiled_trans = tf.transpose(tiled, perm=[1,0,2])\n",
    "    \n",
    "    diffs = tiled - tiled_trans\n",
    "    sum_act = tf.reduce_sum(tf.square(diffs), axis=2)\n",
    "    \n",
    "    return sum_act\n",
    "\n",
    "def _make_Q(output, alpha, batch_size):\n",
    "    \"\"\"\n",
    "    Calculate the \"Q\" probability distribution of the output\n",
    "    Based on the t-distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    output : 2-d Tensor (N, output_dims)\n",
    "        Output of the neural network\n",
    "    alpha : float\n",
    "        `alpha` parameter. Recommend `output_dims` - 1.0\n",
    "    batch_size : int\n",
    "        The batch size. output.shape[0] == batch_size but we need it\n",
    "        provided explicitly\n",
    "    Returns\n",
    "    -------\n",
    "    Q_ : 2-d Tensor (N, N)\n",
    "        Symmetric \"Q\" probability distribution; similarity of\n",
    "        points based on output data\n",
    "    \"\"\"\n",
    "    out_sq_diffs = _get_squared_cross_diff_tf(output)\n",
    "    Q_ = tf.pow((1 + out_sq_diffs/alpha), -(alpha+1)/2)\n",
    "    Q_ = _get_normed_sym_tf(Q_, batch_size)\n",
    "    return Q_\n",
    "\n",
    "def kl_loss(y_true, y_pred, alpha=1.0, batch_size=None, num_perplexities=None, _eps=DEFAULT_EPS):\n",
    "    \"\"\" Kullback-Leibler Loss function (Tensorflow)\n",
    "    between the \"true\" output and the \"predicted\" output\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 2d array_like (N, N*P)\n",
    "        Should be the P matrix calculated from input data.\n",
    "        Differences in input points using a Gaussian probability distribution\n",
    "        Different P (perplexity) values stacked along dimension 1\n",
    "    y_pred : 2d array_like (N, output_dims)\n",
    "        Output of the neural network. We will calculate\n",
    "        the Q matrix based on this output\n",
    "    alpha : float, optional\n",
    "        Parameter used to calculate Q. Default 1.0\n",
    "    batch_size : int, required\n",
    "        Number of samples per batch. y_true.shape[0]\n",
    "    num_perplexities : int, required\n",
    "        Number of perplexities stacked along axis 1\n",
    "    Returns\n",
    "    -------\n",
    "    kl_loss : tf.Tensor, scalar value\n",
    "        Kullback-Leibler divergence P_ || Q_\n",
    "\n",
    "    \"\"\"\n",
    "    P_ = y_true\n",
    "    Q_ = _make_Q(y_pred, alpha, batch_size)\n",
    "    \n",
    "    _tf_eps = tf.constant(_eps, dtype=P_.dtype)\n",
    "    \n",
    "    kls_per_beta = []\n",
    "    components = tf.split(P_, num_perplexities, axis=1, name='split_perp')\n",
    "    for cur_beta_P in components:\n",
    "        #yrange = tf.range(zz*batch_size, (zz+1)*batch_size)\n",
    "        #cur_beta_P = tf.slice(P_, [zz*batch_size, [-1, batch_size])\n",
    "        #cur_beta_P = P_\n",
    "        kl_matr = tf.multiply(cur_beta_P, tf.log(cur_beta_P + _tf_eps) - tf.log(Q_ + _tf_eps), name='kl_matr')\n",
    "        toset = tf.constant(0, shape=[batch_size], dtype=kl_matr.dtype)\n",
    "        kl_matr_keep = tf.matrix_set_diag(kl_matr, toset)\n",
    "        kl_total_cost_cur_beta = tf.reduce_sum(kl_matr_keep)\n",
    "        kls_per_beta.append(kl_total_cost_cur_beta)\n",
    "    kl_total_cost = tf.add_n(kls_per_beta)\n",
    "    #kl_total_cost = kl_total_cost_cur_beta\n",
    "    \n",
    "    return kl_total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 109 ms, sys: 180 ms, total: 289 ms\n",
      "Wall time: 290 ms\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(2000, activation='relu'))\n",
    "model.add(Dense(2))\n",
    "sgd = SGD(lr=0.1) # Stochastic gradient descent optimizer\n",
    "%time model.compile(loss=tsne, optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 2)                 4002      \n",
      "=================================================================\n",
      "Total params: 1,649,002\n",
      "Trainable params: 1,649,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 5000)\n"
     ]
    }
   ],
   "source": [
    "Y_train = P.reshape(X_train.shape[0], -1)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_60 to have shape (None, 2) but got array with shape (60000, 5000)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1572\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1574\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1575\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1409\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1412\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1413\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_60 to have shape (None, 2) but got array with shape (60000, 5000)"
     ]
    }
   ],
   "source": [
    "%time model.fit(X_train, Y_train, batch_size=batch_size, shuffle=False, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_tf = tf.constant(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Pow_10:0\", shape=(60000, 60000), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 60000 and 5000 for 'mul_21' (op: 'Mul') with input shapes: [60000,60000], [5000,5000].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimensions must be equal, but are 60000 and 5000 for 'mul_21' (op: 'Mul') with input shapes: [60000,60000], [5000,5000].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-a7d40e09f4f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtsne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_tf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-fbade00efa2f>\u001b[0m in \u001b[0;36mtsne\u001b[0;34m(P, activations)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mQ\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    892\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1115\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_mul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   2724\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 2726\u001b[0;31m         \"Mul\", x=x, y=y, name=name)\n\u001b[0m\u001b[1;32m   2727\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2728\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local_data/env-py3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 60000 and 5000 for 'mul_21' (op: 'Mul') with input shapes: [60000,60000], [5000,5000]."
     ]
    }
   ],
   "source": [
    "tsne(P, tmp_tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
